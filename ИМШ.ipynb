{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAdyLzjecJ9ls0q0mKW1L8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirrtl/Based-implementation/blob/main/%D0%98%D0%9C%D0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "from einops import rearrange\n",
        "# Датасет\n",
        "class MQARDataset(Dataset):\n",
        "    def __init__(self, vocab_size=100, seq_len=32, min_kv_pairs=2, max_kv_pairs=8, num_samples=1000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.min_kv_pairs = min_kv_pairs\n",
        "        self.max_kv_pairs = max_kv_pairs\n",
        "        self.num_samples = num_samples\n",
        "        self.data = self._generate_data()\n",
        "\n",
        "    def _generate_data(self):\n",
        "        return [self._generate_sequence() for _ in range(self.num_samples)]\n",
        "\n",
        "    def _generate_sequence(self):\n",
        "        num_kv_pairs = random.randint(self.min_kv_pairs, self.max_kv_pairs)\n",
        "        seq = []\n",
        "        kv_store = {}\n",
        "        targets = []\n",
        "\n",
        "        # Генерация key-value пар\n",
        "        for _ in range(num_kv_pairs):\n",
        "            key = random.randint(0, self.vocab_size-1)\n",
        "            value = random.randint(0, self.vocab_size-1)\n",
        "            seq.extend([key, value])\n",
        "            kv_store[key] = value\n",
        "\n",
        "        # Генерация запросов с контролем длины\n",
        "        max_queries = min(len(kv_store), self.seq_len - len(seq))\n",
        "        queries = random.sample(list(kv_store.keys()), max_queries)\n",
        "        for query in queries:\n",
        "            seq.append(query)\n",
        "            targets.append(kv_store[query])\n",
        "\n",
        "        # Обрезка и дополнение последовательности\n",
        "        seq = seq[:self.seq_len]\n",
        "        seq += [random.randint(0, self.vocab_size-1) for _ in range(self.seq_len - len(seq))]\n",
        "        targets = targets[:self.seq_len] + [-100]*(self.seq_len - len(targets))\n",
        "\n",
        "        return torch.tensor(seq), torch.tensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Разложение\n",
        "class TaylorExp(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim  # feature_dim=4\n",
        "        self.r2 = math.sqrt(2)\n",
        "        self.rd = math.sqrt(input_dim)\n",
        "        self.rrd = math.sqrt(math.sqrt(input_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x2 = (x.unsqueeze(-1) * x.unsqueeze(-2)).flatten(start_dim=-2) / self.r2\n",
        "        term1 = torch.ones_like(x[..., :1])  # +1\n",
        "        term2 = x / self.rrd                 # +input_dim\n",
        "        term3 = x2 / self.rd                 # +input_dim²\n",
        "        return torch.cat([term1, term2, term3], dim=-1)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, feature_dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.feature_dim = feature_dim\n",
        "        self.expanded_dim = 1 + feature_dim + feature_dim**2\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.feature_map = TaylorExp(input_dim=self.feature_dim)\n",
        "\n",
        "        self.proj_q = nn.Linear(d_model, self.feature_dim * num_heads)\n",
        "        self.proj_k = nn.Linear(d_model, self.feature_dim * num_heads)\n",
        "        self.proj_v = nn.Linear(d_model, self.head_dim * num_heads)\n",
        "\n",
        "\n",
        "        self.out_proj = nn.Linear(num_heads * self.head_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_state: torch.Tensor, k_state: torch.Tensor):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Проекции\n",
        "        q = rearrange(self.proj_q(x), 'b l (h d) -> b h l d', h=self.num_heads, d=self.feature_dim)\n",
        "        k = rearrange(self.proj_k(x), 'b l (h d) -> b h l d', h=self.num_heads, d=self.feature_dim)\n",
        "        v = rearrange(self.proj_v(x), 'b l (h d) -> b h l d', h=self.num_heads, d=self.head_dim)\n",
        "\n",
        "        # Применение feature map (расширяет до 21)\n",
        "        q = self.feature_map(q)\n",
        "        k = self.feature_map(k)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            # Обновление состояний\n",
        "            kv_update = torch.einsum('b h d, b h c -> b h d c', k[:, :, t], v[:, :, t])\n",
        "            kv_state = kv_state + kv_update\n",
        "            k_state = k_state + k[:, :, t]\n",
        "\n",
        "            # Вычисление внимания\n",
        "            num = torch.einsum('b h d, b h d c -> b h c', q[:, :, t], kv_state)\n",
        "            denom = torch.einsum('b h d, b h d -> b h', q[:, :, t], k_state) + 1e-6\n",
        "            out = num / denom.unsqueeze(-1)\n",
        "            outputs.append(out)\n",
        "\n",
        "        # [seq_len, batch, heads, head_dim] -> [batch, seq_len, heads*head_dim]\n",
        "        x = rearrange(outputs, 't b h d -> b t (h d)')\n",
        "\n",
        "        return self.dropout(self.out_proj(x)), kv_state.detach(), k_state.detach()\n",
        "\n",
        "class SlidingWindow(nn.Module):\n",
        "    def __init__(self, window_size: int, num_heads: int, head_dim: int):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "\n",
        "        self.register_buffer('k_buf', torch.zeros(1, num_heads, window_size, head_dim))\n",
        "        self.register_buffer('v_buf', torch.zeros(1, num_heads, window_size, head_dim))\n",
        "\n",
        "    def forward(self, new_k: torch.Tensor, new_v: torch.Tensor):\n",
        "        # new_k и new_v: [batch, heads, seq_len, head_dim]\n",
        "        batch_size = new_k.size(0)\n",
        "\n",
        "        # Берём последний элемент последовательности\n",
        "        new_k = new_k[:, :, -1:, :]  # [batch, heads, 1, head_dim]\n",
        "        new_v = new_v[:, :, -1:, :]  # [batch, heads, 1, head_dim]\n",
        "\n",
        "        # Расширяем буферы до текущего batch_size\n",
        "        k_buf = self.k_buf.expand(batch_size, -1, -1, -1)\n",
        "        v_buf = self.v_buf.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "        updated_k = torch.cat([k_buf[:, :, 1:], new_k], dim=2)\n",
        "        updated_v = torch.cat([v_buf[:, :, 1:], new_v], dim=2)\n",
        "\n",
        "        # Обновляем буферы с новыми значениями\n",
        "        self.k_buf.data.copy_(updated_k.detach().mean(dim=0, keepdim=True))\n",
        "        self.v_buf.data.copy_(updated_v.detach().mean(dim=0, keepdim=True))\n",
        "\n",
        "        return updated_k, updated_v\n",
        "class BasedBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, window_size: int, feature_dim: int):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.window_size = window_size\n",
        "        self.feature_dim = feature_dim\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.linear_attn = LinearAttention(d_model, feature_dim, num_heads)\n",
        "        self.sliding_window = SlidingWindow(window_size, num_heads, self.head_dim)\n",
        "        self.proj_q = nn.Linear(d_model, d_model)\n",
        "        self.proj_k = nn.Linear(d_model, d_model)\n",
        "        self.proj_v = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def window_attention(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "\n",
        "        q = rearrange(self.proj_q(x), 'b l (h d) -> b h l d', h=self.num_heads)\n",
        "        k = rearrange(self.proj_k(x), 'b l (h d) -> b h l d', h=self.num_heads)\n",
        "        v = rearrange(self.proj_v(x), 'b l (h d) -> b h l d', h=self.num_heads)\n",
        "\n",
        "        window_k, window_v = self.sliding_window(k, v)\n",
        "\n",
        "        scores = torch.einsum('b h q d, b h w d -> b h q w', q, window_k)\n",
        "        attn = torch.softmax(scores / math.sqrt(self.head_dim), dim=-1)\n",
        "        return rearrange(torch.einsum('b h q w, b h w d -> b h q d', attn, window_v),\n",
        "                        'b h l d -> b l (h d)')\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_state: torch.Tensor, k_state: torch.Tensor):\n",
        "        # Линейное внимание\n",
        "        lin_out, kv_state, k_state = self.linear_attn(x, kv_state, k_state)\n",
        "        x = self.norm(x + lin_out)\n",
        "\n",
        "        # Оконное внимание\n",
        "        win_out = self.window_attention(x)\n",
        "        x = self.norm(x + self.dropout(self.out_proj(win_out)))\n",
        "        return x, kv_state, k_state\n",
        "\n",
        "class BasedLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.layers = nn.ModuleList([\n",
        "            BasedBlock(\n",
        "                d_model=config['d_model'],\n",
        "                num_heads=config['num_heads'],\n",
        "                window_size=config['window_size'],\n",
        "                feature_dim=config['feature_dim']\n",
        "            ) for _ in range(config['num_layers'])\n",
        "        ])\n",
        "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'])\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        expanded_dim = self.layers[0].linear_attn.expanded_dim\n",
        "        head_dim = self.layers[0].linear_attn.head_dim\n",
        "\n",
        "        return [(\n",
        "            torch.zeros(batch_size, self.config['num_heads'], expanded_dim, head_dim).to(device),\n",
        "            torch.zeros(batch_size, self.config['num_heads'], expanded_dim).to(device)\n",
        "        ) for _ in self.layers]\n",
        "\n",
        "    def forward(self, input_ids, states=None):\n",
        "        x = self.embed(input_ids)\n",
        "        states = states or self.init_states(input_ids.size(0))\n",
        "        new_states = []\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, *layer_states = layer(x, *states[i])\n",
        "            new_states.append(layer_states)\n",
        "\n",
        "        return self.lm_head(x), new_states\n",
        "\n",
        "\n",
        "def train():\n",
        "    config = {\n",
        "        'vocab_size': 300,\n",
        "        'd_model': 512,\n",
        "        'num_layers': 6,\n",
        "        'num_heads': 8,\n",
        "        'feature_dim': 8,\n",
        "        'window_size': 128,\n",
        "        'batch_size': 5,\n",
        "        'seq_len': 25,\n",
        "        'lr': 8e-4,\n",
        "        'epochs': 2\n",
        "    }\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BasedLM(config).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    dataset = MQARDataset(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        seq_len=config['seq_len']\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(inputs)\n",
        "\n",
        "            # Расчет потерь\n",
        "            loss = criterion(logits.view(-1, config['vocab_size']), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Расчет accuracy\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            mask = targets != -100\n",
        "            correct = (predictions[mask] == targets[mask]).sum().item()\n",
        "            total_correct += correct\n",
        "            total_samples += mask.sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 3 == 0:\n",
        "                accuracy = correct / mask.sum().item() if mask.sum().item() > 0 else 0\n",
        "                print(f'Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f} | Acc: {accuracy:.2%}')\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
        "        print(f'Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_accuracy:.2%}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGpFk3uWXOxE",
        "outputId": "4161bff9-da07-4235-8b8e-64f16127b739"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data validation passed!\n",
            "Epoch 1 | Batch 0 | Loss: 5.7877 | Acc: 0.00%\n",
            "Epoch 1 | Batch 3 | Loss: 6.1658 | Acc: 0.00%\n",
            "Epoch 1 | Batch 6 | Loss: 6.0317 | Acc: 0.00%\n",
            "Epoch 1 | Batch 9 | Loss: 5.8628 | Acc: 0.00%\n",
            "Epoch 1 | Batch 12 | Loss: 5.8222 | Acc: 0.00%\n",
            "Epoch 1 | Batch 15 | Loss: 5.9721 | Acc: 0.00%\n",
            "Epoch 1 | Batch 18 | Loss: 6.2410 | Acc: 0.00%\n",
            "Epoch 1 | Batch 21 | Loss: 6.0413 | Acc: 0.00%\n",
            "Epoch 1 | Batch 24 | Loss: 5.9415 | Acc: 0.00%\n",
            "Epoch 1 | Batch 27 | Loss: 6.1833 | Acc: 0.00%\n",
            "Epoch 1 | Batch 30 | Loss: 6.0934 | Acc: 0.00%\n",
            "Epoch 1 | Batch 33 | Loss: 6.0993 | Acc: 3.70%\n",
            "Epoch 1 | Batch 36 | Loss: 5.9813 | Acc: 0.00%\n",
            "Epoch 1 | Batch 39 | Loss: 5.9019 | Acc: 0.00%\n",
            "Epoch 1 | Batch 42 | Loss: 6.1549 | Acc: 0.00%\n",
            "Epoch 1 | Batch 45 | Loss: 6.0760 | Acc: 0.00%\n",
            "Epoch 1 | Batch 48 | Loss: 6.0383 | Acc: 0.00%\n",
            "Epoch 1 | Batch 51 | Loss: 6.0348 | Acc: 0.00%\n",
            "Epoch 1 | Batch 54 | Loss: 5.9293 | Acc: 0.00%\n",
            "Epoch 1 | Batch 57 | Loss: 6.1612 | Acc: 0.00%\n",
            "Epoch 1 | Batch 60 | Loss: 6.3906 | Acc: 0.00%\n",
            "Epoch 1 | Batch 63 | Loss: 6.2586 | Acc: 0.00%\n",
            "Epoch 1 | Batch 66 | Loss: 6.1616 | Acc: 0.00%\n",
            "Epoch 1 | Batch 69 | Loss: 6.1504 | Acc: 0.00%\n",
            "Epoch 1 | Batch 72 | Loss: 6.1363 | Acc: 0.00%\n",
            "Epoch 1 | Batch 75 | Loss: 5.8857 | Acc: 0.00%\n",
            "Epoch 1 | Batch 78 | Loss: 5.8685 | Acc: 0.00%\n",
            "Epoch 1 | Batch 81 | Loss: 6.0889 | Acc: 0.00%\n",
            "Epoch 1 | Batch 84 | Loss: 6.4055 | Acc: 0.00%\n",
            "Epoch 1 | Batch 87 | Loss: 6.1188 | Acc: 3.85%\n",
            "Epoch 1 | Batch 90 | Loss: 5.9323 | Acc: 0.00%\n",
            "Epoch 1 | Batch 93 | Loss: 6.0698 | Acc: 0.00%\n",
            "Epoch 1 | Batch 96 | Loss: 5.9333 | Acc: 0.00%\n",
            "Epoch 1 | Batch 99 | Loss: 5.9544 | Acc: 0.00%\n",
            "Epoch 1 | Batch 102 | Loss: 5.7226 | Acc: 0.00%\n",
            "Epoch 1 | Batch 105 | Loss: 5.8005 | Acc: 0.00%\n",
            "Epoch 1 | Batch 108 | Loss: 5.9903 | Acc: 0.00%\n",
            "Epoch 1 | Batch 111 | Loss: 6.7593 | Acc: 0.00%\n",
            "Epoch 1 | Batch 114 | Loss: 6.4887 | Acc: 0.00%\n",
            "Epoch 1 | Batch 117 | Loss: 5.9541 | Acc: 0.00%\n",
            "Epoch 1 | Batch 120 | Loss: 6.0984 | Acc: 0.00%\n",
            "Epoch 1 | Batch 123 | Loss: 6.1460 | Acc: 0.00%\n",
            "Epoch 1 | Batch 126 | Loss: 6.1110 | Acc: 0.00%\n",
            "Epoch 1 | Batch 129 | Loss: 5.9529 | Acc: 0.00%\n",
            "Epoch 1 | Batch 132 | Loss: 5.8392 | Acc: 0.00%\n",
            "Epoch 1 | Batch 135 | Loss: 6.0888 | Acc: 0.00%\n",
            "Epoch 1 | Batch 138 | Loss: 5.8361 | Acc: 4.00%\n",
            "Epoch 1 | Batch 141 | Loss: 5.9807 | Acc: 0.00%\n",
            "Epoch 1 | Batch 144 | Loss: 6.1617 | Acc: 0.00%\n",
            "Epoch 1 | Batch 147 | Loss: 5.9865 | Acc: 0.00%\n",
            "Epoch 1 | Batch 150 | Loss: 6.1027 | Acc: 0.00%\n",
            "Epoch 1 | Batch 153 | Loss: 6.1395 | Acc: 3.23%\n",
            "Epoch 1 | Batch 156 | Loss: 5.8791 | Acc: 0.00%\n",
            "Epoch 1 | Batch 159 | Loss: 5.7068 | Acc: 0.00%\n",
            "Epoch 1 | Batch 162 | Loss: 5.9425 | Acc: 0.00%\n",
            "Epoch 1 | Batch 165 | Loss: 5.9339 | Acc: 0.00%\n",
            "Epoch 1 | Batch 168 | Loss: 5.8381 | Acc: 0.00%\n",
            "Epoch 1 | Batch 171 | Loss: 5.9223 | Acc: 0.00%\n",
            "Epoch 1 | Batch 174 | Loss: 5.7511 | Acc: 0.00%\n",
            "Epoch 1 | Batch 177 | Loss: 6.0694 | Acc: 0.00%\n",
            "Epoch 1 | Batch 180 | Loss: 5.8025 | Acc: 0.00%\n",
            "Epoch 1 | Batch 183 | Loss: 5.9711 | Acc: 0.00%\n",
            "Epoch 1 | Batch 186 | Loss: 5.7110 | Acc: 0.00%\n",
            "Epoch 1 | Batch 189 | Loss: 5.9105 | Acc: 0.00%\n",
            "Epoch 1 | Batch 192 | Loss: 6.0619 | Acc: 0.00%\n",
            "Epoch 1 | Batch 195 | Loss: 6.1891 | Acc: 0.00%\n",
            "Epoch 1 | Batch 198 | Loss: 6.0366 | Acc: 0.00%\n",
            "Epoch 1 | Avg Loss: 6.0252 | Avg Acc: 0.18%\n",
            "Epoch 2 | Batch 0 | Loss: 5.7236 | Acc: 0.00%\n",
            "Epoch 2 | Batch 3 | Loss: 6.1250 | Acc: 0.00%\n",
            "Epoch 2 | Batch 6 | Loss: 5.7899 | Acc: 6.25%\n",
            "Epoch 2 | Batch 9 | Loss: 5.8631 | Acc: 0.00%\n",
            "Epoch 2 | Batch 12 | Loss: 5.9392 | Acc: 0.00%\n",
            "Epoch 2 | Batch 15 | Loss: 5.7493 | Acc: 0.00%\n",
            "Epoch 2 | Batch 18 | Loss: 5.8578 | Acc: 3.57%\n",
            "Epoch 2 | Batch 21 | Loss: 5.9232 | Acc: 0.00%\n",
            "Epoch 2 | Batch 24 | Loss: 6.0862 | Acc: 0.00%\n",
            "Epoch 2 | Batch 27 | Loss: 5.8580 | Acc: 0.00%\n",
            "Epoch 2 | Batch 30 | Loss: 6.0228 | Acc: 0.00%\n",
            "Epoch 2 | Batch 33 | Loss: 5.8937 | Acc: 3.70%\n",
            "Epoch 2 | Batch 36 | Loss: 5.9791 | Acc: 0.00%\n",
            "Epoch 2 | Batch 39 | Loss: 5.8210 | Acc: 0.00%\n",
            "Epoch 2 | Batch 42 | Loss: 5.8485 | Acc: 0.00%\n",
            "Epoch 2 | Batch 45 | Loss: 5.6841 | Acc: 4.35%\n",
            "Epoch 2 | Batch 48 | Loss: 5.7400 | Acc: 0.00%\n",
            "Epoch 2 | Batch 51 | Loss: 5.9702 | Acc: 0.00%\n",
            "Epoch 2 | Batch 54 | Loss: 5.8550 | Acc: 0.00%\n",
            "Epoch 2 | Batch 57 | Loss: 5.7156 | Acc: 4.35%\n",
            "Epoch 2 | Batch 60 | Loss: 5.9467 | Acc: 0.00%\n",
            "Epoch 2 | Batch 63 | Loss: 6.0006 | Acc: 0.00%\n",
            "Epoch 2 | Batch 66 | Loss: 5.8365 | Acc: 0.00%\n",
            "Epoch 2 | Batch 69 | Loss: 5.6701 | Acc: 6.67%\n",
            "Epoch 2 | Batch 72 | Loss: 5.8294 | Acc: 0.00%\n",
            "Epoch 2 | Batch 75 | Loss: 5.8460 | Acc: 0.00%\n",
            "Epoch 2 | Batch 78 | Loss: 6.1332 | Acc: 0.00%\n",
            "Epoch 2 | Batch 81 | Loss: 5.9406 | Acc: 0.00%\n",
            "Epoch 2 | Batch 84 | Loss: 5.9737 | Acc: 0.00%\n",
            "Epoch 2 | Batch 87 | Loss: 5.8177 | Acc: 0.00%\n",
            "Epoch 2 | Batch 90 | Loss: 5.9275 | Acc: 0.00%\n",
            "Epoch 2 | Batch 93 | Loss: 5.9965 | Acc: 0.00%\n",
            "Epoch 2 | Batch 96 | Loss: 5.9504 | Acc: 0.00%\n",
            "Epoch 2 | Batch 99 | Loss: 5.8691 | Acc: 0.00%\n",
            "Epoch 2 | Batch 102 | Loss: 5.9977 | Acc: 0.00%\n",
            "Epoch 2 | Batch 105 | Loss: 5.8873 | Acc: 0.00%\n",
            "Epoch 2 | Batch 108 | Loss: 5.9540 | Acc: 0.00%\n",
            "Epoch 2 | Batch 111 | Loss: 5.8440 | Acc: 0.00%\n",
            "Epoch 2 | Batch 114 | Loss: 6.0452 | Acc: 0.00%\n",
            "Epoch 2 | Batch 117 | Loss: 5.8889 | Acc: 0.00%\n",
            "Epoch 2 | Batch 120 | Loss: 5.8415 | Acc: 0.00%\n",
            "Epoch 2 | Batch 123 | Loss: 5.5965 | Acc: 0.00%\n",
            "Epoch 2 | Batch 126 | Loss: 5.6758 | Acc: 0.00%\n",
            "Epoch 2 | Batch 129 | Loss: 5.8485 | Acc: 0.00%\n",
            "Epoch 2 | Batch 132 | Loss: 5.6939 | Acc: 0.00%\n",
            "Epoch 2 | Batch 135 | Loss: 6.0045 | Acc: 0.00%\n",
            "Epoch 2 | Batch 138 | Loss: 5.8292 | Acc: 0.00%\n",
            "Epoch 2 | Batch 141 | Loss: 5.7440 | Acc: 0.00%\n",
            "Epoch 2 | Batch 144 | Loss: 5.8127 | Acc: 0.00%\n",
            "Epoch 2 | Batch 147 | Loss: 5.7622 | Acc: 2.86%\n",
            "Epoch 2 | Batch 150 | Loss: 5.7792 | Acc: 3.23%\n",
            "Epoch 2 | Batch 153 | Loss: 5.8620 | Acc: 0.00%\n",
            "Epoch 2 | Batch 156 | Loss: 5.8711 | Acc: 0.00%\n",
            "Epoch 2 | Batch 159 | Loss: 5.6757 | Acc: 0.00%\n",
            "Epoch 2 | Batch 162 | Loss: 6.2022 | Acc: 0.00%\n",
            "Epoch 2 | Batch 165 | Loss: 6.0322 | Acc: 0.00%\n",
            "Epoch 2 | Batch 168 | Loss: 5.7896 | Acc: 0.00%\n",
            "Epoch 2 | Batch 171 | Loss: 5.9102 | Acc: 0.00%\n",
            "Epoch 2 | Batch 174 | Loss: 5.9808 | Acc: 0.00%\n",
            "Epoch 2 | Batch 177 | Loss: 5.8937 | Acc: 0.00%\n",
            "Epoch 2 | Batch 180 | Loss: 5.6994 | Acc: 0.00%\n",
            "Epoch 2 | Batch 183 | Loss: 5.9317 | Acc: 0.00%\n",
            "Epoch 2 | Batch 186 | Loss: 5.7912 | Acc: 0.00%\n",
            "Epoch 2 | Batch 189 | Loss: 5.8206 | Acc: 0.00%\n",
            "Epoch 2 | Batch 192 | Loss: 5.6700 | Acc: 0.00%\n",
            "Epoch 2 | Batch 195 | Loss: 5.7394 | Acc: 0.00%\n",
            "Epoch 2 | Batch 198 | Loss: 5.7523 | Acc: 0.00%\n",
            "Epoch 2 | Avg Loss: 5.8648 | Avg Acc: 0.33%\n"
          ]
        }
      ]
    }
  ]
}